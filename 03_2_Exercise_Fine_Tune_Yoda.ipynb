{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahabday/DSR-LLM-finetuning/blob/main/03_2_Exercise_Fine_Tune_Yoda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b9b585",
      "metadata": {
        "id": "a8b9b585"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate datasets peft trl bitsandbytes matplotlib gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cbe4193",
      "metadata": {
        "id": "7cbe4193"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "In this exercise, we'll fine-tune Phi-2 to translate sentences from English to the way Yoda talks.\n",
        "\n",
        "In order to accomplish that, we'll create a \"response template\", that is, a special token that triggers the translation. We'll use the token `##[YODA]##>` so, whenever it is added at the end of a sentence, the model should complete it with the translated version.\n",
        "\n",
        "For example, given the prompt:\n",
        "\n",
        "`There is bacon in the sandwich.##[YODA]##>`\n",
        "\n",
        "It should complete the sentence like this:\n",
        "\n",
        "`There is bacon in the sandwich.##[YODA]##>Bacon in the sandwich there is.`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8a9b1fc",
      "metadata": {
        "id": "c8a9b1fc"
      },
      "source": [
        "## Yoda\n",
        "\n",
        "Download the CSV file and load it using [`load_dataset()`](https://huggingface.co/docs/datasets/en/loading). Then, shuffle the dataset and split it into train and test sets ([preprocessing a dataset](https://huggingface.co/docs/datasets/en/process))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c511af8",
      "metadata": {
        "id": "2c511af8"
      },
      "outputs": [],
      "source": [
        "# Downloads yoda_translation.csv\n",
        "!gdown 1luZxKTMuV2E6IGoHI9UARdOFGYAOfBMy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8406a24",
      "metadata": {
        "id": "e8406a24"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Split\n",
        "# load dataset\n",
        "dataset = ...\n",
        "# shuffle and split it\n",
        "dataset = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e76944",
      "metadata": {
        "id": "47e76944"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29dd1201",
      "metadata": {
        "id": "29dd1201"
      },
      "source": [
        "Take a look at one element of the training set. It should have two columns: `sentence` and `yoda` (the translated sentence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b511fdd",
      "metadata": {
        "id": "5b511fdd"
      },
      "outputs": [],
      "source": [
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f2217ac",
      "metadata": {
        "id": "6f2217ac"
      },
      "source": [
        "### Prompt Dataset\n",
        "\n",
        "Now, let's make it a \"prompt dataset\" by renaming the columns to `prompt` and `completion` ([preprocessing a dataset](https://huggingface.co/docs/datasets/en/process)).\n",
        "\n",
        "We'll train the model to take a regular English sentence (the prompt) and produce an output (that is, complete the sentence) with the Yoda translation (completion)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023c23e9",
      "metadata": {
        "id": "023c23e9"
      },
      "outputs": [],
      "source": [
        "# rename the columns\n",
        "prompt_yoda = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c22ac680",
      "metadata": {
        "id": "c22ac680"
      },
      "source": [
        "Take a look at the same element as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b8108d7",
      "metadata": {
        "id": "5b8108d7"
      },
      "outputs": [],
      "source": [
        "prompt_yoda['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc9a7da",
      "metadata": {
        "id": "dbc9a7da"
      },
      "source": [
        "## Tokenizer\n",
        "\n",
        "Use HF's [`AutoTokenizer`](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer) to create a tokenizer for `microsoft/phi-2` model.\n",
        "\n",
        "The parameters for Phi-2 can be found [here](https://huggingface.co/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenTokenizer). Make sure you add a begin of sentence (BOS) and a padding token (`<|pad|>`) as well.\n",
        "\n",
        "We'll need to pad it on the left side (cause we're generating new words starting on the end - the right side). You can force the tokenizer to pad on the left by using `padding_side=\"left\"`. Moreover, we have to set `use_fast=False` because Phi's tokenizer does not support the fast tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f5fcf9c",
      "metadata": {
        "id": "3f5fcf9c"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "base_model_id = 'microsoft/phi-2'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    ...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0574f3b2",
      "metadata": {
        "id": "0574f3b2"
      },
      "source": [
        "Our \"Yoda\" token isn't any of the expected special tokens (padding, unknown, mask, etc.). It is an *additional special token*. Luckily, there is a method to add such tokens to the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f3c587-a584-47aa-a229-5cd4156bb9bb",
      "metadata": {
        "id": "43f3c587-a584-47aa-a229-5cd4156bb9bb"
      },
      "outputs": [],
      "source": [
        "response_template = '##[YODA]##>'\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': [response_template]})\n",
        "\n",
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36b404b1",
      "metadata": {
        "id": "36b404b1"
      },
      "source": [
        "Let's check if the padding and EOS tokens are configured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88721eba-6779-4306-b21d-e6fbb0f70c60",
      "metadata": {
        "id": "88721eba-6779-4306-b21d-e6fbb0f70c60"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token, tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b591b27",
      "metadata": {
        "id": "7b591b27"
      },
      "source": [
        "### Formatting\n",
        "\n",
        "Let's build a formatting function that takes both prompt and completion, and inserts a particular string that will be used to trigger the translation. This string is the response template (`##[YODA]##>`) as previously discussed.\n",
        "\n",
        "The formatting function should produce outputs such as this one:\n",
        "\n",
        "`There is bacon in the sandwich.##[YODA]##>Bacon in the sandwich there is.`\n",
        "\n",
        "However, there is one small - yet important - detail to add: we should add the EOS token to the end of the sentence in order to signal to the model that it should stop the generation at that point.\n",
        "\n",
        "So, the output should really look like this:\n",
        "\n",
        "`There is bacon in the sandwich.##[YODA]##>Bacon in the sandwich there is.<|endoftext|>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "520875d4",
      "metadata": {
        "id": "520875d4"
      },
      "outputs": [],
      "source": [
        "def formatting_func(example):\n",
        "    return ...\n",
        "\n",
        "# Try formatting one example from the training set and see if it is working fine.\n",
        "formatting_func(prompt_yoda['train'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba3cc54",
      "metadata": {
        "id": "0ba3cc54"
      },
      "source": [
        "Now, we'll write a function that takes a prompt, formats it, and tokenizes it. It should truncate the formatted prompt according to the `max_length` argument and, optionally, pad the formatted prompt up to that length (see here the arguments for [calling](https://huggingface.co/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__) a tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c43bb9e",
      "metadata": {
        "id": "0c43bb9e"
      },
      "outputs": [],
      "source": [
        "def generate_and_tokenize_prompt(prompt, max_length=128, padding=True):\n",
        "    result = tokenizer(\n",
        "        ...\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# We'll call it WITHOUT padding first\n",
        "dataset = prompt_yoda['train'].map(lambda v: generate_and_tokenize_prompt(v, padding=False))\n",
        "dataset = dataset.remove_columns(['prompt', 'completion'])\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caacde65",
      "metadata": {
        "id": "caacde65"
      },
      "source": [
        "Perhaps you're wondering where the labels are... as it turns out, the collator will take care of it. We'll be using a collator for completion only, since we're not interested in the regular English sentences that precede our special \"Yoda\" token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b66aa6",
      "metadata": {
        "id": "c9b66aa6"
      },
      "outputs": [],
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "dataloader_completion = DataLoader(dataset, batch_size=2, collate_fn=collator)\n",
        "next(iter(dataloader_completion))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811b0803",
      "metadata": {
        "id": "811b0803"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(tokenized_train_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "190bafb9",
      "metadata": {
        "id": "190bafb9"
      },
      "source": [
        "Now, apply the `generate_and_tokenize_prompt` function to both train and validation sets using the dataset's [`map()`](https://huggingface.co/docs/datasets/en/process#map) method.\n",
        "\n",
        "You can adjust the max length to better match the observed length of the inputs (in the plot above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71156f39",
      "metadata": {
        "id": "71156f39"
      },
      "outputs": [],
      "source": [
        "max_length = ...\n",
        "tokenized_train_dataset = ...\n",
        "tokenized_val_dataset = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0986d61",
      "metadata": {
        "id": "b0986d61"
      },
      "source": [
        "Try tokenizing (and decoding back) one example from the training set. You should see padding tokens to the left of the sentence, an `<|endoftext|>` token signaling the beginning of the sentence (the same token is used both as BOS and EOS token), the original sentence, our special \"Yoda\" token, the translated sentence, and the EOS token at the very end.\n",
        "\n",
        "Here is an example:\n",
        "\n",
        "`'<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|endoftext|>Quench your thirst, then eat the crackers. ##[YODA]##> Quench your thirst, the crackers then eat.<|endoftext|>'`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13753530",
      "metadata": {
        "id": "13753530"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(tokenized_train_dataset[1]['input_ids'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e82d8b0",
      "metadata": {
        "id": "4e82d8b0"
      },
      "source": [
        "## Model\n",
        "\n",
        "Now, let's load the model itself. In order to quantize it while loading it, we need an instance of [`BitAndBytesConfig`](https://huggingface.co/docs/transformers/en/main_classes/quantization#transformers.BitsAndBytesConfig). We can load it in 8-bit using the `NF4` quantization type and double quantization. The computing dtype may be `torch.float16` or - if the GPU supports it - `torch.bfloat16`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df351eb",
      "metadata": {
        "id": "7df351eb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    ...\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36610f62",
      "metadata": {
        "id": "36610f62"
      },
      "source": [
        "Before moving forward, let's check if the embeddings layer needs resizing or not (since we have added a special token to the tokenizer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d9ad190-7305-4fa5-be65-e6684c0f4c69",
      "metadata": {
        "id": "5d9ad190-7305-4fa5-be65-e6684c0f4c69"
      },
      "outputs": [],
      "source": [
        "model.model.embed_tokens, len(tokenizer)\n",
        "# no need\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642187c4-d763-4d1c-9d90-33e440f611b8",
      "metadata": {
        "id": "642187c4-d763-4d1c-9d90-33e440f611b8"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4de141ab",
      "metadata": {
        "id": "4de141ab"
      },
      "source": [
        "How many trainable parameters are left after we load the quantized model? Which layers can still be trained? Let's check it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072778a0",
      "metadata": {
        "id": "072778a0"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model, verbose=False):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            if verbose:\n",
        "                print(name)\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d2b9829-1507-4352-bfd8-3df2f8521507",
      "metadata": {
        "id": "6d2b9829-1507-4352-bfd8-3df2f8521507"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(model, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eed82b8",
      "metadata": {
        "id": "6eed82b8"
      },
      "source": [
        "## LoRA\n",
        "\n",
        "Quantization makes the model smaller to load, but we still need LoRA to make training faster.\n",
        "\n",
        "So, we need to create an instance of [`LoraConfig`](https://huggingface.co/docs/peft/main/en/developer_guides/quantization#loraconfig). You need to choose a rank (`r`), the alpha multiplier (`lora_alpha`), the target modules that will be modified by LoRA (`target_modules`), and - optionally - other modules that should be trained and saved (`modules_to_save`).\n",
        "\n",
        "These extra modules may include layer norm and embeddings modules, for example. Including these modules may deliver better performance but it comes at the cost of not being able to merge multiple adapters together later.\n",
        "\n",
        "Next, you can use the configuration to get the modified model using `get_peft_model()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae5103c",
      "metadata": {
        "id": "2ae5103c"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "prepared_model = ...\n",
        "\n",
        "config = LoraConfig(\n",
        "    ...\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "peft_model = ...\n",
        "peft_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b5abd5-5af2-4e3e-b23f-386fff972db9",
      "metadata": {
        "id": "72b5abd5-5af2-4e3e-b23f-386fff972db9"
      },
      "outputs": [],
      "source": [
        "print_trainable_parameters(peft_model, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c39202a",
      "metadata": {
        "id": "3c39202a"
      },
      "source": [
        "## Training\n",
        "\n",
        "Before actually training the model, we have to configure its training arguments. Hugging Face's `TrainingArguments` is very thorough and comprehensive, so we're providing suggested arguments right away.\n",
        "\n",
        "It is important to notice that:\n",
        "- it uses a paged 8-bit optimizer in order to save memory\n",
        "- it uses gradient accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5571db2b",
      "metadata": {
        "id": "5571db2b"
      },
      "outputs": [],
      "source": [
        "# Some Environment Setup\n",
        "OUTPUT_DIR = \"./results/yoda/\" # the path to the output directory; where model checkpoints will be saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02187851",
      "metadata": {
        "id": "02187851"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        warmup_steps=2,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        auto_find_batch_size=True,\n",
        "        max_steps=500,\n",
        "        learning_rate=2.5e-5,        # Want a small lr for finetuning\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,            # When to start reporting loss\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=500,              # Save checkpoints every 50 steps\n",
        "        eval_strategy=\"steps\",       # Evaluate the model every logging step\n",
        "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training\n",
        "        report_to=\"none\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a60529",
      "metadata": {
        "id": "c2a60529"
      },
      "source": [
        "The `Trainer` object needs:\n",
        "- a model (`model` arg)\n",
        "- a training set (`train_dataset` arg)\n",
        "- an (optional) validation set (`eval_dataset` arg)\n",
        "- the training arguments (`args` arg)\n",
        "- a data collator (`data_collator` arg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2b73a48",
      "metadata": {
        "id": "c2b73a48"
      },
      "source": [
        "Training may take around 10 minutes in an RTX 3090. In Colab's free version, it will take much longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4acbee45",
      "metadata": {
        "id": "4acbee45"
      },
      "outputs": [],
      "source": [
        "trainer = transformers.Trainer(\n",
        "    ...\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dfce950",
      "metadata": {
        "id": "7dfce950"
      },
      "source": [
        "After 1,000 steps, training loss should be around 0.3. So, we save the trained model to disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc8aa532",
      "metadata": {
        "id": "bc8aa532"
      },
      "outputs": [],
      "source": [
        "model_ckpt = OUTPUT_DIR + \"/stop\"\n",
        "\n",
        "trainer.save_model(model_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d310dbb",
      "metadata": {
        "id": "7d310dbb"
      },
      "source": [
        "## Reloading the Model\n",
        "\n",
        "Now, let's reload the trained adapter we have just saved. Remember, it only saves a partial model, so we still need the (quantized) base model.\n",
        "\n",
        "We can use [`PeftModel.from_pretrained()`](https://huggingface.co/docs/peft/en/package_reference/peft_model#peft.PeftModel.from_pretrained) method to load the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f139b83b-1885-499b-af8a-9fdbd474d3b8",
      "metadata": {
        "id": "f139b83b-1885-499b-af8a-9fdbd474d3b8"
      },
      "outputs": [],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "fine_tuned_model = PeftModel.from_pretrained(model, model_ckpt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820f0590",
      "metadata": {
        "id": "820f0590"
      },
      "source": [
        "Now, let's try out our model!\n",
        "\n",
        "First, we'll \"forget\" the response template and see how the model reacts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f0884df-f9ab-48c5-b367-dc6358c037c4",
      "metadata": {
        "id": "8f0884df-f9ab-48c5-b367-dc6358c037c4"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"Luke, I am your father!\"\n",
        "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "fine_tuned_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(fine_tuned_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.1)[0], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76b1de0d",
      "metadata": {
        "id": "76b1de0d"
      },
      "source": [
        "Nothing happened... what if we add the proper response template (`##[YODA]##>`)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d417267a-16ba-4518-b47d-55af2eb2498f",
      "metadata": {
        "id": "d417267a-16ba-4518-b47d-55af2eb2498f"
      },
      "outputs": [],
      "source": [
        "eval_prompt = \"I am your father!\"\n",
        "model_input = tokenizer(eval_prompt+response_template, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "fine_tuned_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(tokenizer.decode(fine_tuned_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.1)[0], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0330e28",
      "metadata": {
        "id": "f0330e28"
      },
      "source": [
        "OK, that's more like it! We got a Yoda-like sentence back!\n",
        "\n",
        "Let's write a function that handles all the boilerplate for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9378e164-2671-4351-9dbc-b05c66e59f87",
      "metadata": {
        "id": "9378e164-2671-4351-9dbc-b05c66e59f87"
      },
      "outputs": [],
      "source": [
        "def generate(model, tokenizer, prompt, response_template=\"\", max_new_tokens=100):\n",
        "    tokenized_input = tokenizer(prompt+response_template, return_tensors=\"pt\")\n",
        "    input_ids = tokenized_input[\"input_ids\"].cuda()\n",
        "\n",
        "    model.eval()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        num_beams=3,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        repetition_penalty=1.1,\n",
        "        do_sample=True, top_p=0.9,temperature=0.95,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    output = tokenizer.batch_decode(generation_output, skip_special_tokens=False)[0]\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6d7d593",
      "metadata": {
        "id": "c6d7d593"
      },
      "source": [
        "Now, let's see our new function in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "961b4634-71ea-4469-82fb-cce2837246c3",
      "metadata": {
        "id": "961b4634-71ea-4469-82fb-cce2837246c3"
      },
      "outputs": [],
      "source": [
        "generate(fine_tuned_model, tokenizer, 'The Force is strong in this one.', response_template, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccaf6ac0-31fc-41eb-8fbc-bb48103c21ca",
      "metadata": {
        "id": "ccaf6ac0-31fc-41eb-8fbc-bb48103c21ca"
      },
      "outputs": [],
      "source": [
        "generate(fine_tuned_model, tokenizer, 'I am coming home.', response_template, max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f3eed62",
      "metadata": {
        "id": "4f3eed62"
      },
      "outputs": [],
      "source": [
        "sample = prompt_yoda['test'][1]\n",
        "generate(fine_tuned_model, tokenizer, sample['prompt'], response_template, max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5d93506",
      "metadata": {
        "id": "e5d93506"
      },
      "source": [
        "Finally, we can *disable* the LoRA adapter we trained to see how the base model reacts to the sample sentence (with and without the response template):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bbcde4f-7e27-4ba1-973e-13ab57d3d556",
      "metadata": {
        "id": "1bbcde4f-7e27-4ba1-973e-13ab57d3d556"
      },
      "outputs": [],
      "source": [
        "with fine_tuned_model.disable_adapter():\n",
        "    print(generate(fine_tuned_model, tokenizer, sample['prompt']))\n",
        "    print(generate(fine_tuned_model, tokenizer, sample['prompt'], response_template))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1248c637-ab9f-4d25-8859-b98ee7285cd9",
      "metadata": {
        "id": "1248c637-ab9f-4d25-8859-b98ee7285cd9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}